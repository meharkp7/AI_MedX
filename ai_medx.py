# -*- coding: utf-8 -*-
"""AI_MedX.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DTUR-iuASf8Sg4XBZVVWonkRFdygY8PP
"""

from IPython import get_ipython
from IPython.display import display
import pandas as pd
import numpy as np
import shap
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor, XGBClassifier # Import XGBClassifier here
from sklearn.multioutput import MultiOutputRegressor
from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler
from imblearn.over_sampling import SMOTE
from scipy.sparse import hstack
from sklearn.metrics import (
    mean_squared_error, accuracy_score, precision_score, recall_score, f1_score, r2_score, roc_auc_score
)

# Load dataset (using usecase_2 instead of usecase_4)
data = pd.read_csv('dataset.csv')

# Rename NCT Number column for consistency
data.rename(columns={'NCT Number': 'nct_id'}, inplace=True)

# Define the combined feature set for all problem statements
features = [
    'nct_id', 'Study Title', 'Study Status', 'Conditions', 'Interventions',
    'Primary Outcome Measures', 'Sponsor', 'Funder Type', 'Sex', 'Phases',
    'Age', 'Enrollment', 'Study Type', 'Study Design', 'Locations', 'Study Recruitment Rate'
]

# Extract relevant features
data = data[features]

# Split structured and unstructured features
numerical_features = ['Study Recruitment Rate', 'Enrollment']
categorical_features = ['Study Status', 'Phases', 'Age', 'Sponsor', 'Funder Type', 'Study Type', 'Sex']
text_features = ['Study Title', 'Conditions', 'Interventions', 'Primary Outcome Measures']

# Handle missing values
data['Study Recruitment Rate'] = pd.to_numeric(data['Study Recruitment Rate'].str.replace(',', ''), errors='coerce')
data['Enrollment'] = pd.to_numeric(data['Enrollment'].str.replace(',', ''), errors='coerce')
data[numerical_features] = data[numerical_features].fillna(0)
data[categorical_features] = data[categorical_features].fillna('')
data[text_features] = data[text_features].fillna('')

scaler = StandardScaler()
encoder = OneHotEncoder(handle_unknown='ignore')

X_numerical = scaler.fit_transform(data[numerical_features])
X_categorical = encoder.fit_transform(data[categorical_features])

X_structured = hstack([X_numerical, X_categorical])

# Preprocessing: Unstructured text data using TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(stop_words='english', max_features=500)
tfidf_matrix = vectorizer.fit_transform(data[text_features].apply(lambda row: ' '.join(row), axis=1))

X_final = hstack([X_structured, tfidf_matrix])

# **ðŸ”¥ Define Targets (PS-2, PS-3, PS-4)**
y = data[['Enrollment', 'Study Status', 'Study Recruitment Rate']].copy()
y.loc[:, 'Study Status'] = y['Study Status'].apply(lambda x: 1 if x == 'COMPLETED' else 0)

# ðŸ”¥ Extract the target variable for SMOTE (Study Status)
y_classification = y['Study Status']

# ðŸ”¥ Ensure y_classification is of type int
y_classification = y_classification.astype(int)

# ðŸ”¥ Apply SMOTE only to the classification target
smote = SMOTE(random_state=42)
X_resampled, y_resampled_classification = smote.fit_resample(X_final.toarray(), y_classification)

# ðŸ”¥ Create a new DataFrame with the resampled data and other target variables
y_resampled = pd.DataFrame(y_resampled_classification, columns=['Study Status'])

# Instead of resetting the index, use the original index from y_classification
y_resampled = y_resampled.reset_index(drop=True)

# Now you can safely access the values using the original index
# Duplicate original values for the upsampled rows
y_resampled['Enrollment'] = np.repeat(y['Enrollment'].values, len(y_resampled) // len(y) + 1)[:len(y_resampled)]
y_resampled['Study Recruitment Rate'] = np.repeat(y['Study Recruitment Rate'].values, len(y_resampled) // len(y) + 1)[:len(y_resampled)]

#ðŸ”¥ Convert `y_resampled` to a NumPy Array (Fixing MultiOutputRegressor Error)**
#y_resampled = y_resampled.values  # Converts DataFrame to 2D NumPy array

# **ðŸ”¥ Train-Test Split**
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

class MultiTaskXGBoost:
    def __init__(self):
        self.regression_model = MultiOutputRegressor(XGBRegressor(n_estimators=200, learning_rate=0.03, random_state=42))
        self.classification_model = XGBClassifier(n_estimators=200, learning_rate=0.03, random_state=42)

    def fit(self, X_train, y_train):
        print(f"\nðŸš€ DEBUG: y_train shape before slicing: {y_train.shape}")

        y_train_np = y_train.values  # âœ… Convert DataFrame to NumPy array

        print(f"ðŸš€ DEBUG: Unique values in y_train[:, 1] (PS-3) before fixing: {np.unique(y_train_np[:, 1])}")  # ðŸ”¥ Print values

        # **ðŸ”¥ Ensure `y_train[:, 1]` contains only 0 & 1**
        y_train_classification = np.where(y_train_np[:, 1] > 1, 1, y_train_np[:, 1]).astype(int)

        print(f"ðŸš€ DEBUG: Unique values in y_train[:, 1] after fixing: {np.unique(y_train_classification)}")  # ðŸ”¥ Confirm values

        print("ðŸš€ Training Regression Model (PS-2 & PS-4)...")
        self.regression_model.fit(X_train, y_train_np[:, [0, 2]])

        print("ðŸš€ Training Classification Model (PS-3)...")
        self.classification_model.fit(X_train, y_train_classification)

    def predict(self, X_test):
        print("ðŸš€ Making Predictions...")
        y_pred_regression = self.regression_model.predict(X_test)
        y_pred_classification = self.classification_model.predict(X_test).reshape(-1, 1)

        return np.hstack([
            y_pred_regression[:, 0].reshape(-1, 1),
            y_pred_classification,
            y_pred_regression[:, 1].reshape(-1, 1)
        ])

model = MultiTaskXGBoost()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
y_pred[:, 1] = (y_pred[:, 1] > 0.5).astype(int)

def evaluate_regression(y_true, y_pred, model_name):
    print("RMSE:", np.sqrt(mean_squared_error(y_true, y_pred)))
    print("RÂ² Score:", r2_score(y_true, y_pred))

def evaluate_classification(y_true, y_pred, model_name):
    print("Accuracy:", accuracy_score(y_true, y_pred))
    print("F1 Score:", f1_score(y_true, y_pred, average='macro'))
    print("Precision:", precision_score(y_true, y_pred, average='macro'))
    print("Recall:", recall_score(y_true, y_pred, average='macro'))
    print("ROC-AUC:", roc_auc_score(y_true, y_pred))
print("PS-2")
print(evaluate_regression(y_test['Enrollment'], y_pred[:, 0], "Enrollment Duration (PS-2) ")) # Changed from y_test[:, 0] to y_test['Enrollment']
print("\nPS-3")
print(evaluate_classification(y_test['Study Status'], y_pred[:, 1], "Trial Completion (PS-3)")) # Changed from y_test[:, 1] to y_test['Study Status']
print("\nPS-4")
print(evaluate_regression(y_test['Study Recruitment Rate'], y_pred[:, 2], "Recruitment Rate (PS-4)")) # Changed from y_test[:, 2] to y_test['Study Recruitment Rate']

shap_values_reg2 = shap.Explainer(model.regression_model.estimators_[0], X_train).shap_values(X_test)
shap_values_reg4 = shap.Explainer(model.regression_model.estimators_[1], X_train).shap_values(X_test)

explainer_class = shap.Explainer(model.classification_model, X_train)
shap_values_class = explainer_class.shap_values(X_test)

shap.summary_plot(shap_values_reg2, X_test)

shap.summary_plot(shap_values_class, X_test)

shap.summary_plot(shap_values_reg4, X_test)

sample_idx = 10

shap.force_plot(explainer_class.expected_value, shap_values_reg2[sample_idx, :], X_test[sample_idx, :], matplotlib=True)

shap.force_plot(explainer_class.expected_value, shap_values_class[sample_idx], X_test[sample_idx], matplotlib=True)

shap.force_plot(explainer_reg.expected_value, shap_values_reg4[sample_idx, :], X_test[sample_idx, :], matplotlib=True)

plt.figure(figsize=(10, 6))
shap.summary_plot(shap_values_reg, X_test)
plt.title("SHAP Feature Importance for Regression Model")
plt.show()

plt.figure(figsize=(20, 12))
xgb_importance = model.regression_model.estimators_[0].feature_importances_
plt.barh(range(len(xgb_importance)), xgb_importance)

all_feature_names = numerical_features + list(encoder.get_feature_names_out(categorical_features)) + [f"text_{i}" for i in range(tfidf_matrix.shape[1])]
plt.yticks(range(len(xgb_importance)), all_feature_names)
plt.xlabel("Feature Importance")
plt.ylabel("Features")
plt.title("XGBoost Feature Importance for Regression Model")
plt.show()

plt.figure(figsize=(10, 6))
xgb_importance_class = model.classification_model.feature_importances_
plt.barh(range(len(xgb_importance_class)), xgb_importance_class)

all_feature_names = numerical_features + list(encoder.get_feature_names_out(categorical_features)) + [f"text_{i}" for i in range(tfidf_matrix.shape[1])]
plt.yticks(range(len(xgb_importance_class)), all_feature_names)

plt.xlabel("Feature Importance")
plt.ylabel("Features")
plt.title("XGBoost Feature Importance for Classification Model")
plt.show()

regression_scores = np.array([
    [np.sqrt(mean_squared_error(y_test['Enrollment'], y_pred[:, 0]))],
    [r2_score(y_test['Enrollment'], y_pred[:, 0])],
    [np.sqrt(mean_squared_error(y_test['Study Recruitment Rate'], y_pred[:, 2]))],
    [r2_score(y_test['Study Recruitment Rate'], y_pred[:, 2])]
])

plt.figure(figsize=(6, 3))
sns.heatmap(regression_scores, annot=True, cmap="coolwarm", xticklabels=["RMSE / RÂ²"], yticklabels=["PS-2 RMSE", "PS-2 RÂ²", "PS-4 RMSE", "PS-4 RÂ²"])
plt.title("Regression Model Performance")
plt.show()

classification_scores = np.array([
    [accuracy_score(y_test['Study Status'], y_pred[:, 1])],
    [precision_score(y_test['Study Status'], y_pred[:, 1], average='macro')],
    [recall_score(y_test['Study Status'], y_pred[:, 1], average='macro')],
    [f1_score(y_test['Study Status'], y_pred[:, 1], average='macro')],
    [roc_auc_score(y_test['Study Status'], y_pred[:, 1])]
])

plt.figure(figsize=(6, 4))
sns.heatmap(classification_scores, annot=True, cmap="coolwarm", xticklabels=["Score"], yticklabels=["Accuracy", "Precision", "Recall", "F1", "ROC-AUC"])
plt.title("Classification Model Performance")
plt.show()

from sklearn.metrics import confusion_matrix, precision_recall_curve
precision, recall, _ = precision_recall_curve(y_test['Study Status'], y_pred[:, 1])

plt.figure(figsize=(8, 6))
plt.plot(recall, precision, marker='.', label='PS-3 Classification Model')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.grid()
plt.show()